# Vector Embeddings Demo z FastAPI, PostgreSQL i pgvector

To jest projekt edukacyjny, kt√≥ry demonstruje:
- **Embeddingi wektorowe** u≈ºywajƒÖc **lokalnego modelu sentence-transformers** (bez klucza API!)
- **Wyszukiwanie semantyczne** z pgvector w PostgreSQL
- **JSONB** dla elastycznych metadanych dokument√≥w
- **FastAPI** dla REST API
- **n8n** dla automatyzacji workflow z ingestion artyku≈Ç√≥w przez webhook
- **Logowanie aplikacji** do ≈õledzenia wszystkich operacji

Wszystkie us≈Çugi dzia≈ÇajƒÖ w Docker dla ≈Çatwego lokalnego developmentu.

---

## Funkcjonalno≈õci

‚úÖ **Chunking gotowy dla RAG** - Dokumenty dzielone na fragmenty ~60 s≈Ç√≥w dla precyzyjnego wyszukiwania  
‚úÖ **Wsparcie wielojƒôzyczne** - Dzia≈Ça z polskim, angielskim i 50+ jƒôzykami  
‚úÖ **Bez koszt√≥w API** - U≈ºywa lokalnego modelu `paraphrase-multilingual-MiniLM-L12-v2`  
‚úÖ **Wykrywanie duplikat√≥w** - Automatycznie zapobiega duplikacji URL  
‚úÖ **Ingestion przez webhook** - Dodawaj artyku≈Çy przez workflow n8n  
‚úÖ **Logowanie aplikacji** - ≈öled≈∫ wszystkie operacje w `api/app.log`  
‚úÖ **Wyszukiwanie semantyczne** - Znajd≈∫ relevantne fragmenty tekstu, nie tylko s≈Çowa kluczowe  

---

## Wymagania

- macOS z zainstalowanym i uruchomionym Docker Desktop
- **Bez klucza API!** Wszystko dzia≈Ça lokalnie.

---

## Instalacja

### 1. Skonfiguruj zmienne ≈õrodowiskowe (opcjonalne)

Plik `.env` jest opcjonalny, poniewa≈º u≈ºywamy lokalnych embedding√≥w:

```bash
cp .env.example .env
```

### 2. Zbuduj i uruchom stack Docker

Z tego katalogu uruchom:

```bash
docker compose up --build
```

To uruchomi trzy us≈Çugi:
- **PostgreSQL** z rozszerzeniem pgvector (port 5432)
- **FastAPI** backend (port 8000)
- **n8n** automatyzacja workflow (port 5678)

Poczekaj a≈º zobaczysz logi wskazujƒÖce, ≈ºe wszystkie us≈Çugi sƒÖ gotowe (zazwyczaj 20-30 sekund).

---

## Us≈Çugi

Po uruchomieniu mo≈ºesz uzyskaƒá dostƒôp do:

### FastAPI API
- **URL**: http://localhost:8000
- **Interaktywna dokumentacja**: http://localhost:8000/docs (Swagger UI)
- **Alternatywna dokumentacja**: http://localhost:8000/redoc

### n8n Automatyzacja Workflow
- **URL**: http://localhost:5678
- **Login**: 
  - Nazwa u≈ºytkownika: `admin`
  - Has≈Ço: `admin`

---

## Testowanie API

### Przyk≈Çad 1: Dodaj dokument

Dodaj dokument o kotach i psach (po polsku):

```bash
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Artyku≈Ç o kotach i psach",
    "body": "Kot ≈õpi na kanapie, a pies biega po ogrodzie. Koty sƒÖ leniwymi zwierzƒôtami.",
    "metadata": {"category": "animals", "lang": "pl"}
  }'
```

Odpowied≈∫:
```json
{
  "status": "ok",
  "document_id": 1,
  "chunks_inserted": 1
}
```

Dodaj kolejny dokument:

```bash
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Przepis na ciasto",
    "body": "Wymieszaj mƒÖkƒô, cukier i jajka. Piecz w 180 stopniach przez 30 minut.",
    "metadata": {"category": "recipes", "lang": "pl"}
  }'
```

I jeszcze jeden (po angielsku):

```bash
curl -X POST http://localhost:8000/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Article about dogs",
    "body": "Dogs are loyal companions. They love to play and run in the park.",
    "metadata": {"category": "animals", "lang": "en"}
  }'
```

### Przyk≈Çad 2: Wyszukiwanie semantyczne

Wyszukaj dokumenty o kotach (po polsku):

```bash
curl "http://localhost:8000/search?q=kot&limit=3"
```

Lub o psach (po angielsku):

```bash
curl "http://localhost:8000/search?q=dog&limit=3"
```

Lub o przepisach/gotowaniu:

```bash
curl "http://localhost:8000/search?q=gotowanie&limit=3"
```

API zwr√≥ci dokumenty posortowane wed≈Çug podobie≈Ñstwa semantycznego (najni≈ºsza odleg≈Ço≈õƒá = najbardziej podobne).

### Przyk≈Çad 3: Lista wszystkich dokument√≥w

```bash
curl http://localhost:8000/documents
```

---

## U≈ºywanie Workflow n8n

Projekt zawiera gotowe do u≈ºycia workflow n8n dla automatycznej ingestion artyku≈Ç√≥w.

### Import Workflow

1. Otw√≥rz n8n: http://localhost:5678 (login: `admin` / `admin`)
2. Kliknij **trzy kropki** (prawy g√≥rny r√≥g) ‚Üí **Import from File**
3. Zaimportuj pliki z `n8n_workflows/`:
   - `1_ingest_from_url.json` - Pobierz i dodaj artyku≈Çy z URL
   - `2_search_documents.json` - Przeszukuj bazƒô wektorowƒÖ

### U≈ºywanie Workflow Webhook

Workflow **Ingest from URL** akceptuje requesty POST:

**URL**: `http://localhost:5678/webhook/ingest-url`  
**Metoda**: `POST`  
**Body**:
```json
{
  "url": "https://example.com/article"
}
```

**Przyk≈Çad z curl**:
```bash
curl -X POST http://localhost:5678/webhook/ingest-url \
  -H "Content-Type: application/json" \
  -d '{"url": "https://weszlo.com/sylvinho-trener-reprezentacji-albanii-to-on-sprobuje-ograc-urbana/"}'
```

Workflow:
1. Pobierze HTML z URL
2. WyciƒÖgnie tytu≈Ç i paragrafy
3. Wy≈õle do vector API
4. **Pominie je≈õli URL ju≈º istnieje** (wykrywanie duplikat√≥w)

Zobacz `docs/workflows/N8N_WORKFLOW_DOCS.md` dla szczeg√≥≈Çowej dokumentacji n8n.

---

## Logowanie Aplikacji

Wszystkie operacje sƒÖ logowane do `api/app.log`. Zobacz logi:

```bash
cat api/app.log
```

Lub w czasie rzeczywistym:
```bash
tail -f api/app.log
```

Wpisy log√≥w zawierajƒÖ:
- Ingestion dokument√≥w (tytu≈Ç, metadata, URL)
- Wykrywanie duplikat√≥w (pominiƒôte URL)
- Zapytania wyszukiwania (tekst zapytania, liczba wynik√≥w)
- B≈Çƒôdy i ostrze≈ºenia

---

## Jak To Dzia≈Ça

### 1. Ingestion Dokument√≥w (`POST /ingest`)

Kiedy wysy≈Çasz dokument:
1. **Aplikacja FastAPI** sprawdza czy dokument z tym samym URL ju≈º istnieje
2. Je≈õli istnieje, zwraca istniejƒÖcy document ID (bez duplikatu)
3. W przeciwnym razie, **body dokumentu jest dzielone na chunki** (~60 s≈Ç√≥w ka≈ºdy, 15 s≈Ç√≥w nak≈Çadki)
4. Ka≈ºdy chunk otrzymuje sw√≥j w≈Çasny **384-wymiarowy embedding wektorowy** u≈ºywajƒÖc lokalnego modelu `paraphrase-multilingual-MiniLM-L12-v2`
5. Chunki sƒÖ przechowywane w **PostgreSQL** z:
   - `title` (dziedziczone z dokumentu)
   - `body` (tekst chunka, ~60 s≈Ç√≥w)
   - `metadata` jako **JSONB** (elastyczne przechowywanie JSON)
   - `embedding` jako **vector(384)** w kolumnie pgvector
   - `document_id` (link do dokumentu nadrzƒôdnego)
   - `chunk_index` (pozycja w dokumencie)

**Dlaczego chunking?** To umo≈ºliwia RAG (Retrieval-Augmented Generation) poprzez zwracanie precyzyjnych, relevantnych fragment√≥w tekstu zamiast ca≈Çych dokument√≥w.

### 2. Wyszukiwanie Semantyczne (`GET /search`)

Kiedy wyszukujesz:
1. Tekst zapytania jest konwertowany na wektor u≈ºywajƒÖc **lokalnego modelu wielojƒôzycznego** (bez wywo≈Çania API!)
2. PostgreSQL przeszukuje **tabelƒô chunks** u≈ºywajƒÖc **operatora `<->`** (odleg≈Ço≈õƒá L2) aby znale≈∫ƒá chunki z podobnymi embeddingami
3. Wyniki sƒÖ sortowane wed≈Çug odleg≈Ço≈õci (ni≈ºsza = bardziej podobne)
4. API zwraca najbardziej relevantne **fragmenty tekstu** (chunki), nie ca≈Çe dokumenty

**To oznacza, ≈ºe wyszukiwanie rozumie znaczenie**, nie tylko s≈Çowa kluczowe! Na przyk≈Çad, wyszukiwanie "karmienie kota" znajdzie konkretne chunki o karmieniu kot√≥w, nawet je≈õli dok≈Çadna fraza siƒô r√≥≈ºni.

**Idealne dla RAG:** Ka≈ºdy wynik to fragment ~60 s≈Ç√≥w, kt√≥ry mo≈ºe byƒá bezpo≈õrednio u≈ºyty jako kontekst dla LLM.

### 3. Metadata JSONB

Ka≈ºdy dokument mo≈ºe mieƒá elastyczne metadata przechowywane jako JSONB. To pozwala na:
- Szybkie zapytania na polach JSON (u≈ºywajƒÖc indeksu GIN)
- Elastyczny schemat (nie trzeba predefiniowaƒá wszystkich p√≥l)
- Przyk≈Çadowe zapytania SQL: 
  ```sql
  SELECT * FROM documents WHERE metadata->>'category' = 'animals';
  ```

### 4. pgvector

Rozszerzenie `pgvector` dodaje:
- Typ danych `vector` do przechowywania embedding√≥w
- Operatory odleg≈Ço≈õci: `<->` (L2), `<=>` (cosine), `<#>` (iloczyn skalarny)
- Specjalizowane indeksy (IVFFlat, HNSW) dla szybkiego wyszukiwania podobie≈Ñstwa

---

## Struktura Projektu

```
/Users/brakuzy/Code/personal/vector/
‚îú‚îÄ‚îÄ docker-compose.yml       # Definiuje 3 us≈Çugi (db, api, n8n)
‚îú‚îÄ‚îÄ .env.example             # Szablon zmiennych ≈õrodowiskowych
‚îú‚îÄ‚îÄ .env                     # Twoja aktualna konfiguracja (git-ignored, opcjonalne)
‚îú‚îÄ‚îÄ .gitignore               # Regu≈Çy git ignore
‚îú‚îÄ‚îÄ README.md                # Ten plik
‚îú‚îÄ‚îÄ CHANGELOG.md             # Historia zmian
‚îú‚îÄ‚îÄ docs/                    # Dokumentacja projektu
‚îÇ   ‚îú‚îÄ‚îÄ README.md            # Indeks dokumentacji
‚îÇ   ‚îú‚îÄ‚îÄ technical/           # Dokumentacja techniczna
‚îÇ   ‚îú‚îÄ‚îÄ workflows/           # Dokumentacja n8n
‚îÇ   ‚îî‚îÄ‚îÄ guides/              # Przewodniki u≈ºytkownika
‚îú‚îÄ‚îÄ n8n_workflows/           # Gotowe do importu workflow n8n
‚îÇ   ‚îú‚îÄ‚îÄ 1_ingest_from_url.json
‚îÇ   ‚îî‚îÄ‚îÄ 2_search_documents.json
‚îî‚îÄ‚îÄ api/
    ‚îú‚îÄ‚îÄ Dockerfile           # Buduje kontener FastAPI
    ‚îú‚îÄ‚îÄ requirements.txt     # Zale≈ºno≈õci Python
    ‚îî‚îÄ‚îÄ app/
        ‚îú‚îÄ‚îÄ main.py          # Aplikacja FastAPI z endpointami
        ‚îú‚îÄ‚îÄ db.py            # Helper po≈ÇƒÖczenia z bazƒÖ danych
        ‚îú‚îÄ‚îÄ chunking.py      # Algorytm chunkowania tekstu
        ‚îî‚îÄ‚îÄ app.log          # Logi aplikacji (auto-tworzone)
```

---

## Zatrzymywanie Us≈Çug

Naci≈õnij `Ctrl+C` w terminalu gdzie dzia≈Ça `docker compose`, lub uruchom:

```bash
docker compose down
```

Aby usunƒÖƒá wszystkie dane (baza danych + wolumeny n8n):

```bash
docker compose down -v
```

---

## Chunking & RAG

Ta implementacja u≈ºywa **chunkowania tekstu** aby umo≈ºliwiƒá RAG (Retrieval-Augmented Generation):

- **Rozmiar chunka**: 60 s≈Ç√≥w (konfigurowalne w `api/app/chunking.py`)
- **Nak≈Çadka**: 15 s≈Ç√≥w (zachowuje kontekst miƒôdzy chunkami)
- **Korzy≈õci**: Zwraca precyzyjne, relevantne fragmenty (2-3 zdania) zamiast ca≈Çych dokument√≥w

### Dlaczego Chunking?

**Bez chunkowania:**
- Wyszukiwanie zwraca ca≈Çy artyku≈Ç 5000 s≈Ç√≥w
- Relevantna informacja zakopana w ≈õrodku
- Za du≈ºo tekstu dla kontekstu LLM

**Z chunkowaniem:**
- Wyszukiwanie zwraca konkretny fragment 60 s≈Ç√≥w
- Precyzyjne dopasowanie semantyczne
- Idealny rozmiar dla kontekstu LLM
- Mo≈ºliwe wiele relevantnych chunk√≥w z tego samego dokumentu

### Zmiana Rozmiaru Chunka

Edytuj `api/app/chunking.py`:
```python
def chunk_text(text: str, chunk_size: int = 60, overlap: int = 15):
    # Dostosuj chunk_size i overlap wed≈Çug potrzeb
```

---

## Wsparcie Wielojƒôzyczne

Projekt u≈ºywa `paraphrase-multilingual-MiniLM-L12-v2` kt√≥ry wspiera 50+ jƒôzyk√≥w w tym:
- Polski
- Angielski
- Niemiecki, Francuski, Hiszpa≈Ñski, W≈Çoski
- I wiele wiƒôcej

### Zmiana Modelu

Aby u≈ºyƒá innego modelu, edytuj `api/app/main.py`:

```python
# Obecny: Wielojƒôzyczny (50+ jƒôzyk√≥w, 384 wymiary)
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# Alternatywa: Tylko angielski (szybszy, mniejszy)
model = SentenceTransformer("all-MiniLM-L6-v2")

# Alternatywa: Najlepsza jako≈õƒá wielojƒôzyczna (768 wymiar√≥w - wymaga zmiany schematu DB!)
model = SentenceTransformer("paraphrase-multilingual-mpnet-base-v2")
```

**Uwaga:** Je≈õli zmieniasz na model z innymi wymiarami (np. 768), musisz r√≥wnie≈º zaktualizowaƒá `vector(384)` na `vector(768)` w `api/app/db.py` i przebudowaƒá bazƒô danych.

---

## Nastƒôpne Kroki

Teraz gdy masz dzia≈ÇajƒÖcƒÖ konfiguracjƒô, mo≈ºesz:

1. **Przetestowaƒá workflow RAG**: Dodaj d≈Çugie artyku≈Çy, wyszukuj konkretne tematy, u≈ºywaj wynik√≥w z LLM
2. **Eksperymentowaƒá z r√≥≈ºnymi tekstami**: Spr√≥buj dodaƒá dokumenty w r√≥≈ºnych jƒôzykach
3. **Testowaƒá wyszukiwanie semantyczne**: Zauwa≈º jak znajduje podobne znaczenie, nie tylko pasujƒÖce s≈Çowa
4. **Eksplorowaƒá zapytania JSONB**: Dodaj bardziej z≈Ço≈ºone metadata i zapytuj je bezpo≈õrednio w PostgreSQL
5. **Uczyƒá siƒô n8n**: Tw√≥rz workflow, kt√≥re automatycznie dodajƒÖ dokumenty z zewnƒôtrznych ≈∫r√≥de≈Ç
6. **Dostosowaƒá rozmiar chunka**: Eksperymentuj z r√≥≈ºnymi rozmiarami chunk√≥w dla swojego przypadku u≈ºycia
7. **Wypr√≥bowaƒá r√≥≈ºne modele**: Testuj modele tylko angielskie vs wielojƒôzyczne

---

## Przydatne Komendy

### Zobacz logi
```bash
docker compose logs -f api    # Logi FastAPI
docker compose logs -f db     # Logi PostgreSQL
docker compose logs -f n8n    # Logi n8n
```

### Zobacz logi aplikacji
```bash
cat api/app.log               # Wszystkie logi
tail -f api/app.log           # ≈öled≈∫ logi w czasie rzeczywistym
```

### Po≈ÇƒÖcz siƒô bezpo≈õrednio z PostgreSQL
```bash
docker exec -it vector_db psql -U app -d app
```

Nastƒôpnie mo≈ºesz uruchomiƒá zapytania SQL:
```sql
-- Zobacz tabelƒô documents
SELECT * FROM documents;

-- Sprawd≈∫ duplikaty
SELECT metadata->>'url' as url, COUNT(*) 
FROM documents 
WHERE metadata->>'url' IS NOT NULL 
GROUP BY metadata->>'url' 
HAVING COUNT(*) > 1;

-- Wyszukaj u≈ºywajƒÖc SQL bezpo≈õrednio
SELECT id, title, embedding <-> '[0.1, 0.2, ...]'::vector AS distance
FROM chunks
ORDER BY distance
LIMIT 5;
```

### Przebuduj po zmianach w kodzie
```bash
docker compose up --build
```

---

## RozwiƒÖzywanie Problem√≥w

**Problem**: ≈Åadowanie modelu trwa d≈Çugo przy pierwszym starcie

**RozwiƒÖzanie**: To normalne - model (~90MB) jest pobierany przy pierwszym uruchomieniu. Kolejne starty sƒÖ znacznie szybsze.

**Problem**: B≈Çƒôdy po≈ÇƒÖczenia z bazƒÖ danych

**RozwiƒÖzanie**: Poczekaj trochƒô d≈Çu≈ºej na inicjalizacjƒô PostgreSQL, lub zrestartuj: `docker compose restart api`

**Problem**: n8n siƒô nie ≈Çaduje

**RozwiƒÖzanie**: Daj mu minutƒô - n8n trwa trochƒô d≈Çu≈ºej przy pierwszym uruchomieniu

**Problem**: Duplikaty dokument√≥w w bazie danych

**RozwiƒÖzanie**: API teraz automatycznie zapobiega duplikatom na podstawie URL. IstniejƒÖce duplikaty mo≈ºna usunƒÖƒá przez SQL.

---

## Dokumentacja

üìö **[Indeks Dokumentacji](docs/README.md)** - Ca≈Ça dokumentacja uporzƒÖdkowana wed≈Çug kategorii

### Szybkie Linki

- **[Dokumentacja Techniczna](docs/technical/DOKUMENTACJA_TECHNICZNA.md)** - Kompleksowy przewodnik techniczny
- **[Workflow n8n](docs/workflows/N8N_WORKFLOW_DOCS.md)** - Przewodnik automatyzacji workflow
- **[Przewodnik Chunkowania](docs/guides/CHUNKING_GUIDE.md)** - Jak dzia≈Ça chunking tekstu
- **[Przewodnik Wyszukiwania](docs/guides/SEARCH_ENDPOINT_GUIDE.md)** - Dokumentacja endpointu search
- **[Changelog](CHANGELOG.md)** - Historia wersji

---

Mi≈Çej nauki! üöÄ
